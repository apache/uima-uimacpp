   #alternate Edit this page Wikipedia (en) Wikipedia Atom feed

   Jump to content

   [ ] Main menu
   Main menu
   (BUTTON) move to sidebar (BUTTON) hide
   Navigation
     * Main page
     * Contents
     * Current events
     * Random article
     * About Wikipedia
     * Contact us
     * Donate

   Contribute
     * Help
     * Learn to edit
     * Community portal
     * Recent changes
     * Upload file

   Languages
   Language links are at the top of the page across from the title.
   Wikipedia The Free Encyclopedia
   Search
   ____________________
   (BUTTON) Search

     * Create account
     * Log in

   [ ] Personal tools
     * Create account
     * Log in

   Pages for logged out editors learn more
     * Contributions
     * Talk

   [ ]

Contents

   (BUTTON) move to sidebar (BUTTON) hide
     * (Top)
     * 1Description
       (BUTTON) Toggle Description subsection
          + 1.1Software
          + 1.2Hardware
          + 1.3Data
     * 2Operation
       (BUTTON) Toggle Operation subsection
          + 2.1Comparison with human players
     * 3History
       (BUTTON) Toggle History subsection
          + 3.1Development
          + 3.2Jeopardy!
               o 3.2.1Preparation
               o 3.2.2Practice match
               o 3.2.3First match
               o 3.2.4Second match
               o 3.2.5Final outcome
               o 3.2.6Philosophy
               o 3.2.7Match against members of the United States Congress
     * 4Current and future applications
       (BUTTON) Toggle Current and future applications subsection
          + 4.1Healthcare
          + 4.2IBM Watson Group
          + 4.3Chef Watson
          + 4.4Chatbot
          + 4.5Building codes
          + 4.6Teaching assistant
          + 4.7Weather forecasting
          + 4.8Fashion
          + 4.9Tax preparation
          + 4.10Advertising
     * 5See also
     * 6References
     * 7Bibliography
     * 8Further reading
     * 9External links
       (BUTTON) Toggle External links subsection
          + 9.1J! Archive
          + 9.2Videos

   Toggle the table of contents

   [ ] Toggle the table of contents

IBM Watson

   [ ] 33 languages
     * العربية
     * Български
     * Català
     * Deutsch
     * Eesti
     * Español
     * Euskara
     * فارسی
     * Français
     * 한국어
     * Հայերեն
     * Italiano
     * עברית
     * Magyar
     * മലയാളം
     * မြန်မာဘာသာ
     * Nederlands
     * 日本語
     * Norsk bokmål
     * Oʻzbekcha / ўзбекча
     * Polski
     * Português
     * Română
     * Русский
     * Simple English
     * Slovenčina
     * Suomi
     * Svenska
     * Türkçe
     * Українська
     * Tiếng Việt
     * 吴语
     * 中文

   Edit links

     * Article
     * Talk

   [ ] English

     * Read
     * Edit
     * View history

   [ ] Tools
   Tools
   (BUTTON) move to sidebar (BUTTON) hide
   Actions
     * Read
     * Edit
     * View history

   General
     * What links here
     * Related changes
     * Upload file
     * Special pages
     * Permanent link
     * Page information
     * Cite this page
     * Wikidata item

   Print/export
     * Download as PDF
     * Printable version

   In other projects
     * Wikimedia Commons

   From Wikipedia, the free encyclopedia
   Artificial intelligence computer system made by IBM
   For the IBM laboratory, see Thomas J. Watson Research Center.
   [220px-IBM_Watson_Logo_2017.png] Watson's avatar, inspired by the IBM
   "Smarter Planet" logo

   IBM Watson is a question-answering computer system capable of answering
   questions posed in natural language,^[1] developed in IBM's DeepQA
   project by a research team led by principal investigator David
   Ferrucci.^[2] Watson was named after IBM's founder and first CEO,
   industrialist Thomas J. Watson.^[3]^[4]

   The computer system was initially developed to answer questions on the
   quiz show Jeopardy!^[5] and in 2011, the Watson computer system
   competed on Jeopardy! against champions Brad Rutter and Ken
   Jennings,^[3]^[6] winning the first place prize of 1 million USD.^[7]

   In February 2013, IBM announced that Watson's first commercial
   application would be for utilization management decisions in lung
   cancer treatment at Memorial Sloan Kettering Cancer Center, New York
   City, in conjunction with WellPoint (now Elevance Health).^[8].

Description[edit]

   [220px-DeepQA.svg.png] The high-level architecture of IBM's DeepQA used
   in Watson^[9]

   Watson was created as a question answering (QA) computing system that
   IBM built to apply advanced natural language processing, information
   retrieval, knowledge representation, automated reasoning, and machine
   learning technologies to the field of open domain question
   answering.^[1]

   IBM stated that Watson uses "more than 100 different techniques to
   analyze natural language, identify sources, find and generate
   hypotheses, find and score evidence, and merge and rank
   hypotheses."^[10]

   In recent years, Watson's capabilities have been extended and the way
   in which Watson works has been changed to take advantage of new
   deployment models (Watson on IBM Cloud), evolved machine learning
   capabilities, and optimized hardware available to developers and
   researchers. It is no longer purely a question answering (QA) computing
   system designed from Q&A pairs but can now 'see', 'hear', 'read',
   'talk', 'taste', 'interpret', 'learn' and 'recommend'.^[citation
   needed]

Software[edit]

   Watson uses IBM's DeepQA software and the Apache UIMA (Unstructured
   Information Management Architecture) framework implementation. The
   system was written in various languages, including Java, C++, and
   Prolog, and runs on the SUSE Linux Enterprise Server 11 operating
   system using the Apache Hadoop framework to provide distributed
   computing.^[11]^[12]^[13]

Hardware[edit]

   The system is workload-optimized, integrating massively parallel POWER7
   processors and built on IBM's DeepQA technology,^[14] which it uses to
   generate hypotheses, gather massive evidence, and analyze data.^[1]
   Watson employs a cluster of ninety IBM Power 750 servers, each of which
   uses a 3.5 GHz POWER7 eight-core processor, with four threads per core.
   In total, the system has 2,880 POWER7 processor threads and 16
   terabytes of RAM.^[14]

   According to John Rennie, Watson can process 500 gigabytes (the
   equivalent of a million books) per second.^[15] IBM master inventor and
   senior consultant Tony Pearson estimated Watson's hardware cost at
   about three million dollars.^[16] Its Linpack performance stands at 80
   TeraFLOPs, which is about half as fast as the cut-off line for the Top
   500 Supercomputers list.^[17] According to Rennie, all content was
   stored in Watson's RAM for the Jeopardy game because data stored on
   hard drives would be too slow to compete with human Jeopardy
   champions.^[15]

Data[edit]

   The sources of information for Watson include encyclopedias,
   dictionaries, thesauri, newswire articles and literary works. Watson
   also used databases, taxonomies and ontologies including DBPedia,
   WordNet and Yago.^[18] The IBM team provided Watson with millions of
   documents, including dictionaries, encyclopedias and other reference
   material, that it could use to build its knowledge.^[19]

Operation[edit]

   Watson parses questions into different keywords and sentence fragments
   in order to find statistically related phrases.^[19] Watson's main
   innovation was not in the creation of a new algorithm for this
   operation but rather its ability to quickly execute hundreds of proven
   language analysis algorithms simultaneously.^[19]^[20] The more
   algorithms that find the same answer independently, the more likely
   Watson is to be correct. Once Watson has a small number of potential
   solutions, it is able to check against its database to ascertain
   whether the solution makes sense or not.^[19]

Comparison with human players[edit]

   [220px-Watson_Jeopardy.jpg] Ken Jennings, Watson, and Brad Rutter in
   their Jeopardy! exhibition match

   Watson's basic working principle is to parse keywords in a clue while
   searching for related terms as responses. This gives Watson some
   advantages and disadvantages compared with human Jeopardy!
   players.^[21] Watson has deficiencies in understanding the contexts of
   the clues. Watson can read, analyze, and learn from natural language
   which gives it the ability to make human-like decisions.^[22] As a
   result, human players usually generate responses faster than Watson,
   especially to short clues.^[19] Watson's programming prevents it from
   using the popular tactic of buzzing before it is sure of its
   response.^[19] However, Watson has consistently better reaction time on
   the buzzer once it has generated a response, and is immune to human
   players' psychological tactics, such as jumping between categories on
   every clue.^[19]^[23]

   In a sequence of 20 mock games of Jeopardy, human participants were
   able to use the six to seven seconds that Watson needed to hear the
   clue and decide whether to signal for responding.^[19] During that
   time, Watson also has to evaluate the response and determine whether it
   is sufficiently confident in the result to signal.^[19] Part of the
   system used to win the Jeopardy! contest was the electronic circuitry
   that receives the "ready" signal and then examined whether Watson's
   confidence level was great enough to activate the buzzer. Given the
   speed of this circuitry compared to the speed of human reaction times,
   Watson's reaction time was faster than the human contestants except
   when the human anticipated (instead of reacted to) the ready
   signal.^[24] After signaling, Watson speaks with an electronic voice
   and gives the responses in Jeopardy!'s question format.^[19] Watson's
   voice was synthesized from recordings that actor Jeff Woodman made for
   an IBM text-to-speech program in 2004.^[25]

   The Jeopardy! staff used different means to notify Watson and the human
   players when to buzz,^[24] which was critical in many rounds.^[23] The
   humans were notified by a light, which took them tenths of a second to
   perceive.^[26]^[27] Watson was notified by an electronic signal and
   could activate the buzzer within about eight milliseconds.^[28] The
   humans tried to compensate for the perception delay by anticipating the
   light,^[29] but the variation in the anticipation time was generally
   too great to fall within Watson's response time.^[23] Watson did not
   attempt to anticipate the notification signal.^[27]^[29]

History[edit]

Development[edit]

   Since Deep Blue's victory over Garry Kasparov in chess in 1997, IBM had
   been on the hunt for a new challenge. In 2004, IBM Research manager
   Charles Lickel, over dinner with coworkers, noticed that the restaurant
   they were in had fallen silent. He soon discovered the cause of this
   evening's hiatus: Ken Jennings, who was then in the middle of his
   successful 74-game run on Jeopardy!. Nearly the entire restaurant had
   piled toward the televisions, mid-meal, to watch Jeopardy!. Intrigued
   by the quiz show as a possible challenge for IBM, Lickel passed the
   idea on, and in 2005, IBM Research executive Paul Horn supported
   Lickel, pushing for someone in his department to take up the challenge
   of playing Jeopardy! with an IBM system. Though he initially had
   trouble finding any research staff willing to take on what looked to be
   a much more complex challenge than the wordless game of chess,
   eventually David Ferrucci took him up on the offer.^[30] In
   competitions managed by the United States government, Watson's
   predecessor, a system named Piquant, was usually able to respond
   correctly to only about 35% of clues and often required several minutes
   to respond.^[31]^[32]^[33] To compete successfully on Jeopardy!, Watson
   would need to respond in no more than a few seconds, and at that time,
   the problems posed by the game show were deemed to be impossible to
   solve.^[19]

   In initial tests run during 2006 by David Ferrucci, the senior manager
   of IBM's Semantic Analysis and Integration department, Watson was given
   500 clues from past Jeopardy! programs. While the best real-life
   competitors buzzed in half the time and responded correctly to as many
   as 95% of clues, Watson's first pass could get only about 15% correct.
   During 2007, the IBM team was given three to five years and a staff of
   15 people to solve the problems.^[19] John E. Kelly III succeeded Paul
   Horn as head of IBM Research in 2007.^[34] InformationWeek described
   Kelly as "the father of Watson" and credited him for encouraging the
   system to compete against humans on Jeopardy!.^[35] By 2008, the
   developers had advanced Watson such that it could compete with
   Jeopardy! champions.^[19] By February 2010, Watson could beat human
   Jeopardy! contestants on a regular basis.^[36]

   During the game, Watson had access to 200 million pages of structured
   and unstructured content consuming four terabytes of disk storage^[11]
   including the full text of the 2011 edition of Wikipedia,^[37] but was
   not connected to the Internet.^[38]^[19] For each clue, Watson's three
   most probable responses were displayed on the television screen. Watson
   consistently outperformed its human opponents on the game's signaling
   device, but had trouble in a few categories, notably those having short
   clues containing only a few words.

   Although the system is primarily an IBM effort, Watson's development
   involved faculty and graduate students from Rensselaer Polytechnic
   Institute, Carnegie Mellon University, University of Massachusetts
   Amherst, the University of Southern California's Information Sciences
   Institute, the University of Texas at Austin, the Massachusetts
   Institute of Technology, and the University of Trento,^[9] as well as
   students from New York Medical College.^[39] Among the team of IBM
   programmers who worked on Watson was Ed Toutant, who himself had
   appeared on Jeopardy! in 1989 (winning one game).^[40]

Jeopardy![edit]

Preparation[edit]

   [220px-IBMWatson.jpg] Watson demo at an IBM booth at a trade show

   In 2008, IBM representatives communicated with Jeopardy! executive
   producer Harry Friedman about the possibility of having Watson compete
   against Ken Jennings and Brad Rutter, two of the most successful
   contestants on the show, and the program's producers agreed.^[19]^[41]
   Watson's differences with human players had generated conflicts between
   IBM and Jeopardy! staff during the planning of the competition.^[21]
   IBM repeatedly expressed concerns that the show's writers would exploit
   Watson's cognitive deficiencies when writing the clues, thereby turning
   the game into a Turing test. To alleviate that claim, a third party
   randomly picked the clues from previously written shows that were never
   broadcast.^[21] Jeopardy! staff also showed concerns over Watson's
   reaction time on the buzzer. Originally Watson signaled electronically,
   but show staff requested that it press a button physically, as the
   human contestants would.^[42] Even with a robotic "finger" pressing the
   buzzer, Watson remained faster than its human competitors. Ken Jennings
   noted, "If you're trying to win on the show, the buzzer is all", and
   that Watson "can knock out a microsecond-precise buzz every single time
   with little or no variation. Human reflexes can't compete with computer
   circuits in this regard."^[23]^[29]^[43] Stephen Baker, a journalist
   who recorded Watson's development in his book Final Jeopardy, reported
   that the conflict between IBM and Jeopardy! became so serious in May
   2010 that the competition was almost cancelled.^[21] As part of the
   preparation, IBM constructed a mock set in a conference room at one of
   its technology sites to model the one used on Jeopardy!. Human players,
   including former Jeopardy! contestants, also participated in mock games
   against Watson with Todd Alan Crain of The Onion playing host.^[19]
   About 100 test matches were conducted with Watson winning 65% of the
   games.^[44]

   To provide a physical presence in the televised games, Watson was
   represented by an "avatar" of a globe, inspired by the IBM "smarter
   planet" symbol. Jennings described the computer's avatar as a "glowing
   blue ball crisscrossed by 'threads' of thought—42 threads, to be
   precise",^[45] and stated that the number of thought threads in the
   avatar was an in-joke referencing the significance of the number 42 in
   Douglas Adams' Hitchhiker's Guide to the Galaxy.^[45] Joshua Davis, the
   artist who designed the avatar for the project, explained to Stephen
   Baker that there are 36 trigger-able states that Watson was able to use
   throughout the game to show its confidence in responding to a clue
   correctly; he had hoped to be able to find forty-two, to add another
   level to the Hitchhiker's Guide reference, but he was unable to
   pinpoint enough game states.^[46]

   A practice match was recorded on January 13, 2011, and the official
   matches were recorded on January 14, 2011. All participants maintained
   secrecy about the outcome until the match was broadcast in
   February.^[47]

Practice match[edit]

   In a practice match before the press on January 13, 2011, Watson won a
   15-question round against Ken Jennings and Brad Rutter with a score of
   $4,400 to Jennings's $3,400 and Rutter's $1,200, though Jennings and
   Watson were tied before the final $1,000 question. None of the three
   players responded incorrectly to a clue.^[48]

First match[edit]

   The first round was broadcast February 14, 2011, and the second round,
   on February 15, 2011. The right to choose the first category had been
   determined by a draw won by Rutter.^[49] Watson, represented by a
   computer monitor display and artificial voice, responded correctly to
   the second clue and then selected the fourth clue of the first
   category, a deliberate strategy to find the Daily Double as quickly as
   possible.^[50] Watson's guess at the Daily Double location was correct.
   At the end of the first round, Watson was tied with Rutter at $5,000;
   Jennings had $2,000.^[49]

   Watson's performance was characterized by some quirks. In one instance,
   Watson repeated a reworded version of an incorrect response offered by
   Jennings. (Jennings said "What are the '20s?" in reference to the
   1920s. Then Watson said "What is 1920s?") Because Watson could not
   recognize other contestants' responses, it did not know that Jennings
   had already given the same response. In another instance, Watson was
   initially given credit for a response of "What is a leg?" after
   Jennings incorrectly responded "What is: he only had one hand?" to a
   clue about George Eyser (the correct response was, "What is: he's
   missing a leg?"). Because Watson, unlike a human, could not have been
   responding to Jennings's mistake, it was decided that this response was
   incorrect. The broadcast version of the episode was edited to omit
   Trebek's original acceptance of Watson's response.^[51] Watson also
   demonstrated complex wagering strategies on the Daily Doubles, with one
   bet at $6,435 and another at $1,246.^[52] Gerald Tesauro, one of the
   IBM researchers who worked on Watson, explained that Watson's wagers
   were based on its confidence level for the category and a complex
   regression model called the Game State Evaluator.^[53]

   Watson took a commanding lead in Double Jeopardy!, correctly responding
   to both Daily Doubles. Watson responded to the second Daily Double
   correctly with a 32% confidence score.^[52]

   However, during the Final Jeopardy! round, Watson was the only
   contestant to miss the clue in the category U.S. Cities ("Its largest
   airport was named for a World War II hero; its second largest, for a
   World War II battle"). Rutter and Jennings gave the correct response of
   Chicago, but Watson's response was "What is Toronto?????" with five
   question marks appended indicating a lack of confidence.^[52]^[54]^[55]
   Ferrucci offered reasons why Watson would appear to have guessed a
   Canadian city: categories only weakly suggest the type of response
   desired, the phrase "U.S. city" did not appear in the question, there
   are cities named Toronto in the U.S., and Toronto in Ontario has an
   American League baseball team.^[56] Chris Welty, who also worked on
   Watson, suggested that it may not have been able to correctly parse the
   second part of the clue, "its second largest, for a World War II
   battle" (which was not a standalone clause despite it following a
   semicolon, and required context to understand that it was referring to
   a second-largest airport).^[57] Eric Nyberg, a professor at Carnegie
   Mellon University and a member of the development team, stated that the
   error occurred because Watson does not possess the comparative
   knowledge to discard that potential response as not viable.^[55]
   Although not displayed to the audience as with non-Final Jeopardy!
   questions, Watson's second choice was Chicago. Both Toronto and Chicago
   were well below Watson's confidence threshold, at 14% and 11%
   respectively. Watson wagered only $947 on the question.

   The game ended with Jennings with $4,800, Rutter with $10,400, and
   Watson with $35,734.^[52]

Second match[edit]

   During the introduction, Trebek (a Canadian native) joked that he had
   learned Toronto was a U.S. city, and Watson's error in the first match
   prompted an IBM engineer to wear a Toronto Blue Jays jacket to the
   recording of the second match.^[58]

   In the first round, Jennings was finally able to choose a Daily Double
   clue,^[59] while Watson responded to one Daily Double clue incorrectly
   for the first time in the Double Jeopardy! Round.^[60] After the first
   round, Watson placed second for the first time in the competition after
   Rutter and Jennings were briefly successful in increasing their dollar
   values before Watson could respond.^[60]^[61] Nonetheless, the final
   result ended with a victory for Watson with a score of $77,147, besting
   Jennings who scored $24,000 and Rutter who scored $21,600.^[62]

Final outcome[edit]

   The prizes for the competition were $1 million for first place
   (Watson), $300,000 for second place (Jennings), and $200,000 for third
   place (Rutter). As promised, IBM donated 100% of Watson's winnings to
   charity, with 50% of those winnings going to World Vision and 50% going
   to World Community Grid.^[63] Similarly, Jennings and Rutter donated
   50% of their winnings to their respective charities.^[64]

   In acknowledgement of IBM and Watson's achievements, Jennings made an
   additional remark in his Final Jeopardy! response: "I for one welcome
   our new computer overlords."^[65]^[66] Jennings later wrote an article
   for Slate, in which he stated:

     IBM has bragged to the media that Watson's question-answering skills
     are good for more than annoying Alex Trebek. The company sees a
     future in which fields like medical diagnosis, business analytics,
     and tech support are automated by question-answering software like
     Watson. Just as factory jobs were eliminated in the 20th century by
     new assembly-line robots, Brad and I were the first
     knowledge-industry workers put out of work by the new generation of
     'thinking' machines. 'Quiz show contestant' may be the first job
     made redundant by Watson, but I'm sure it won't be the last.^[45]

Philosophy[edit]

   Philosopher John Searle argues that Watson—despite impressive
   capabilities—cannot actually think.^[67] Drawing on his Chinese room
   thought experiment, Searle claims that Watson, like other computational
   machines, is capable only of manipulating symbols, but has no ability
   to understand the meaning of those symbols; however, Searle's
   experiment has its detractors.^[68]

Match against members of the United States Congress[edit]

   On February 28, 2011, Watson played an untelevised exhibition match of
   Jeopardy! against members of the United States House of
   Representatives. In the first round, Rush D. Holt, Jr. (D-NJ, a former
   Jeopardy! contestant), who was challenging the computer with Bill
   Cassidy (R-LA, later Senator from Louisiana), led with Watson in second
   place. However, combining the scores between all matches, the final
   score was $40,300 for Watson and $30,000 for the congressional players
   combined.^[69]

   IBM's Christopher Padilla said of the match, "The technology behind
   Watson represents a major advancement in computing. In the
   data-intensive environment of government, this type of technology can
   help organizations make better decisions and improve how government
   helps its citizens."^[69]

Current and future applications[edit]

   This section contains content that is written like an advertisement.
   Please help improve it by removing promotional content and
   inappropriate external links, and by adding encyclopedic content
   written from a neutral point of view. (April 2019) (Learn how and when
   to remove this template message)

   According to IBM, "The goal is to have computers start to interact in
   natural human terms across a range of applications and processes,
   understanding the questions that humans ask and providing answers that
   humans can understand and justify."^[36] It has been suggested by
   Robert C. Weber, IBM's general counsel, that Watson may be used for
   legal research.^[70] The company also intends to use Watson in other
   information-intensive fields, such as telecommunications, financial
   services, and government.^[71]

   Watson is based on commercially available IBM Power 750 servers that
   have been marketed since February 2010.^[19]

   Commentator Rick Merritt said that "there's another really important
   reason why it is strategic for IBM to be seen very broadly by the
   American public as a company that can tackle tough computer problems. A
   big slice of [IBM's profit] comes from selling to the U.S. government
   some of the biggest, most expensive systems in the world."^[72]

   In 2013, it was reported that three companies were working with IBM to
   create apps embedded with Watson technology. Fluid is developing an app
   for retailers, one called "The North Face", which is designed to
   provide advice to online shoppers. Welltok is developing an app
   designed to give people advice on ways to engage in activities to
   improve their health. MD Buyline is developing an app for the purpose
   of advising medical institutions on equipment procurement
   decisions.^[73]^[74]

   In November 2013, IBM announced it would make Watson's API available to
   software application providers, enabling them to build apps and
   services that are embedded in Watson's capabilities. To build out its
   base of partners who create applications on the Watson platform, IBM
   consults with a network of venture capital firms, which advise IBM on
   which of their portfolio companies may be a logical fit for what IBM
   calls the Watson Ecosystem. Thus far, roughly 800 organizations and
   individuals have signed up with IBM, with interest in creating
   applications that could use the Watson platform.^[75]

   On January 30, 2013, it was announced that Rensselaer Polytechnic
   Institute would receive a successor version of Watson, which would be
   housed at the Institute's technology park and be available to
   researchers and students.^[76] By summer 2013, Rensselaer had become
   the first university to receive a Watson computer.^[77]

   On February 6, 2014, it was reported that IBM plans to invest $100
   million in a 10-year initiative to use Watson and other IBM
   technologies to help countries in Africa address development problems,
   beginning with healthcare and education.^[78]

   On June 3, 2014, three new Watson Ecosystem partners were chosen from
   more than 400 business concepts submitted by teams spanning 18
   industries from 43 countries. "These bright and enterprising
   organizations have discovered innovative ways to apply Watson that can
   deliver demonstrable business benefits", said Steve Gold, vice
   president, IBM Watson Group. The winners were Majestyk Apps with their
   adaptive educational platform, FANG (Friendly Anthropomorphic Networked
   Genome);^[79]^[80] Red Ant with their retail sales trainer;^[81] and
   GenieMD^[82] with their medical recommendation service.^[83]

   On July 9, 2014, Genesys Telecommunications Laboratories announced
   plans to integrate Watson to improve their customer experience
   platform, citing the sheer volume of customer data to analyze.^[84]

   Watson has been integrated with databases including Bon Appétit
   magazine to perform a recipe generating platform.^[85]

   Watson is being used by Decibel, a music discovery startup, in its app
   MusicGeek which uses the supercomputer to provide music recommendations
   to its users. The use of Watson has also been found in the hospitality
   industry. Go Moment uses Watson for its Rev1 app, which gives hotel
   staff a way to quickly respond to questions from guests.^[86] Arria NLG
   has built an app that helps energy companies stay within regulatory
   guidelines, making it easier for managers to make sense of thousands of
   pages of legal and technical jargon.

   OmniEarth, Inc. uses Watson computer vision services to analyze
   satellite and aerial imagery, along with other municipal data, to infer
   water usage on a property-by-property basis, helping districts in
   California improve water conservation efforts.^[87]

   In September 2016, Condé Nast started using Watson to help build and
   strategize social influencer campaigns for brands. Using software built
   by IBM and Influential, Condé Nast's clients will be able to know which
   influencer's demographics, personality traits and more best align with
   a marketer and the audience it is targeting.^[88]

   In February 2017, Rare Carat, a New York City-based startup and
   e-commerce platform for buying diamonds and diamond rings, introduced
   an IBM Watson-powered artificial intelligence chatbot called "Rocky" to
   assist novice diamond buyers through the daunting process of purchasing
   a diamond. As part of the IBM Global Entrepreneur Program, Rare Carat
   received the assistance of IBM in the development of the Rocky Chat
   Bot.^[89]^[90]^[91] In May 2017, IBM partnered with the Pebble Beach
   Company to use Watson as a concierge.^[92] Watson's artificial
   intelligence was added to an app developed by Pebble Beach and was used
   to guide visitors around the resort. The mobile app was designed by IBM
   iX and hosted on the IBM Cloud. It uses Watson's Conversation
   applications programming interface.

   In November 2017, in Mexico City, the Experience Voices of Another Time
   was opened at the National Museum of Anthropology using IBM Watson as
   an alternative to visiting a museum.^[93]

Healthcare[edit]

   See also: IBM Watson Health

   In healthcare, Watson has been used to analyze medical data and assist
   doctors in making diagnoses and treatment decisions, including in areas
   such as oncology and radiology.^[94] Watson's natural language,
   hypothesis generation, and evidence-based learning capabilities are
   being investigated to see how Watson may contribute to clinical
   decision support systems and the increase in artificial intelligence in
   healthcare for use by medical professionals.^[95] To aid physicians in
   the treatment of their patients, once a physician has posed a query to
   the system describing symptoms and other related factors, Watson first
   parses the input to identify the most important pieces of information;
   then mines patient data to find facts relevant to the patient's medical
   and hereditary history; then examines available data sources to form
   and test hypotheses;^[95] and finally provides a list of
   individualized, confidence-scored recommendations.^[96] The sources of
   data that Watson uses for analysis can include treatment guidelines,
   electronic medical record data, notes from healthcare providers,
   research materials, clinical studies, journal articles and patient
   information.^[95] Despite being developed and marketed as a "diagnosis
   and treatment advisor", Watson has never been actually involved in the
   medical diagnosis process, only in assisting with identifying treatment
   options for patients who have already been diagnosed.^[97]

   In February 2011, it was announced that IBM would be partnering with
   Nuance Communications for a research project to develop a commercial
   product during the next 18 to 24 months, designed to exploit Watson's
   clinical decision support capabilities. Physicians at Columbia
   University would help to identify critical issues in the practice of
   medicine where the system's technology may be able to contribute, and
   physicians at the University of Maryland would work to identify the
   best way that a technology like Watson could interact with medical
   practitioners to provide the maximum assistance.^[98]

   In September 2011, IBM and WellPoint (now Anthem) announced a
   partnership to utilize Watson to help suggest treatment options to
   physicians.^[99] Then, in February 2013, IBM and WellPoint gave Watson
   its first commercial application, for utilization management decisions
   in lung cancer treatment at Memorial Sloan–Kettering Cancer Center.^[8]

   IBM announced a partnership with Cleveland Clinic in October 2012. The
   company has sent Watson to the Cleveland Clinic Lerner College of
   Medicine of Case Western Reserve University, where it will increase its
   health expertise and assist medical professionals in treating patients.
   The medical facility will utilize Watson's ability to store and process
   large quantities of information to help speed up and increase the
   accuracy of the treatment process. "Cleveland Clinic's collaboration
   with IBM is exciting because it offers us the opportunity to teach
   Watson to 'think' in ways that have the potential to make it a powerful
   tool in medicine", said C. Martin Harris, MD, chief information officer
   of Cleveland Clinic.^[100]

   In 2013, IBM and MD Anderson Cancer Center began a pilot program to
   further the center's "mission to eradicate cancer".^[101]^[102]
   However, after spending $62 million, the project did not meet its goals
   and it has been stopped.^[103]

   On February 8, 2013, IBM announced that oncologists at the Maine Center
   for Cancer Medicine and Westmed Medical Group in New York have started
   to test Watson in an effort to recommend treatment for lung
   cancer.^[104]

   On July 29, 2016, IBM and Manipal Hospitals^[105]^[106]^[107] (a
   leading hospital chain in India) announced the launch of IBM Watson for
   Oncology, for cancer patients. This product provides information and
   insights to physicians and cancer patients to help them identify
   personalized, evidence-based cancer care options. Manipal Hospitals is
   the second hospital^[108] in the world to adopt this technology and
   first in the world to offer it to patients online as an expert second
   opinion through their website.^[105]^[109] Manipal discontinued this
   contract in December 2018.

   On January 7, 2017, IBM and Fukoku Mutual Life Insurance entered into a
   contract for IBM to deliver analysis to compensation payouts via its
   IBM Watson Explorer AI, which resulted in the loss of 34 jobs. The
   company said it would speed up compensation payout analysis via
   analyzing claims and medical records, and increase productivity by 30%.
   The company also said it would save ¥140m in running costs.^[110]

   Several startups in the healthcare space have been effectively using
   seven business model archetypes to take solutions based on IBM Watson
   to the marketplace. These archetypes depends on the value generate for
   the target user (e.g. patient focus vs. healthcare provider and payer
   focus) and value capturing mechanisms (e.g. providing information or
   connecting stakeholders).^[111]

   By 2022, IBM Watson Health was generating about a billion dollars in
   annual gross revenue,^[112] but was facing a lack of profitability and
   increased competition. One expert assessed to CNN that "IBM was clearly
   not gaining much traction in the healthcare market". A 2021 post from
   the Association for Computing Machinery (ACM) titled "What Happened To
   Watson Health?" described the portfolio management challenges of IBM
   Watson Health given the number of acquisitions involved in the Watson
   Health division creation in 2015, as well as technical limitations that
   existed at the time regarding where the Watson AI framework could be
   deployed.^[113] In February 2021, the Wall Street Journal reported that
   Watson Health was exploring a sale.^[114] On January 21, 2022, IBM
   announced the sell-off of its Watson Health unit to Francisco
   Partners.^[115]

IBM Watson Group[edit]

   On January 9, 2014, IBM announced it was creating a business unit
   around Watson, led by senior vice president Michael Rhodin.^[116] IBM
   Watson Group will have headquarters in New York's Silicon Alley and
   will employ 2,000 people. IBM has invested $1 billion to get the
   division going. Watson Group will develop three new cloud-delivered
   services: Watson Discovery Advisor, Watson Engagement Advisor, and
   Watson Explorer. Watson Discovery Advisor will focus on research and
   development projects in pharmaceutical industry, publishing, and
   biotechnology, Watson Engagement Advisor will focus on self-service
   applications using insights on the basis of natural language questions
   posed by business users, and Watson Explorer will focus on helping
   enterprise users uncover and share data-driven insights based on
   federated search more easily.^[116] The company is also launching a
   $100 million venture fund to spur application development for
   "cognitive" applications. According to IBM, the cloud-delivered
   enterprise-ready Watson has seen its speed increase 24 times over—a
   2,300 percent improvement in performance and its physical size shrank
   by 90 percent—from the size of a master bedroom to three stacked pizza
   boxes.^[116] IBM CEO Virginia Rometty said she wants Watson to generate
   $10 billion in annual revenue within ten years.^[117] In 2017, IBM and
   MIT established a new joint research venture in artificial
   intelligence. IBM invested $240 million to create the MIT–IBM Watson AI
   Lab in partnership with MIT, which brings together researchers in
   academia and industry to advance AI research, with projects ranging
   from computer vision and NLP to devising new ways to ensure that AI
   systems are fair, reliable and secure.^[118] In March 2018, IBM's CEO
   Ginni Rometty proposed "Watson's Law," the "use of and application of
   business, smart cities, consumer applications and life in
   general."^[119]

Chef Watson[edit]

   Watson helped a team of chefs create five new poutines for the 2015 La
   Poutine Week food festival in Toronto and Montreal. It analyzed the
   demographics and popular cuisines of the cities and drew from a
   database of tens of thousands of recipes to create fusion pairings for
   each city.^[120] IBM and Bon Appétit magazine co-created an AI cooking
   app known as Chef Watson.^[121]

Chatbot[edit]

   Watson is being used via IBM partner program as a chatbot to provide
   the conversation for children's toys.^[122]

Building codes[edit]

   In 2015, the engineering firm ENGEO created an online service via the
   IBM partner program named GoFetchCode. GoFetchCode applies Watson's
   natural language processing and question-answering capabilities to the
   International Code Council's model building codes.^[123]

Teaching assistant[edit]

   IBM Watson is being used for several projects relating to education,
   and has entered partnerships with Pearson Education, Blackboard, Sesame
   Workshop and Apple.^[124]^[125]

   In its partnership with Pearson, Watson is being made available inside
   electronic text books to provide natural language, one-on-one tutoring
   to students on the reading material.^[126]

   As an individual using the free Watson APIs available to the public,
   Ashok Goel, a professor at Georgia Tech, used Watson to create a
   virtual teaching assistant to assist students in his class.^[127]
   Initially, Goel did not reveal the nature of "Jill", which was created
   with the help of a few students and IBM. Jill answered questions where
   it had a 97% certainty of an accurate answer, with the remainder being
   answered by human assistants.^[128]

   The research group of Sabri Pllana developed an assistant for learning
   parallel programming using the IBM Watson.^[129] A survey with a number
   of novice parallel programmers at the Linnaeus University indicated
   that such assistants will be welcomed by students that learn parallel
   programming.

Weather forecasting[edit]

   In August 2016, IBM announced it would be using Watson for weather
   forecasting.^[130] Specifically, the company announced they would use
   Watson to analyze data from over 200,000 Weather Underground personal
   weather stations, as well as data from other sources, as a part of
   Project Deep Thunder.^[131]

Fashion[edit]

   IBM Watson together with Marchesa designed a dress that changed the
   colour of the fabric depending on the mood of the audience. The dress
   lit up in different colours based on the sentiment of Tweets about the
   dress. Tweets were passed through a Watson tone analyzer and then sent
   back to a small computer inside the waist of the dress.^[132]

Tax preparation[edit]

   On February 5–6, 2017, tax preparation company H&R Block began
   nationwide use of a Watson-based program.^[133]

Advertising[edit]

   In September 2017, IBM announced that with its acquisition of The
   Weather Company's advertising sales division, and a partnership with
   advertising neural network Cognitiv, Watson will provide AI-powered
   advertising solutions.^[134]^[135]^[136]

See also[edit]

     * Artificial intelligence
     * Blue Gene
     * Commonsense knowledge (artificial intelligence)
     * Glossary of artificial intelligence
     * Artificial general intelligence
     * Tech companies in the New York metropolitan area
     * Wolfram Alpha

References[edit]

    1. ^ ^a ^b ^c "DeepQA Project: FAQ". IBM. Archived from the original
       on June 29, 2011. Retrieved February 11, 2011.
    2. ^ Ferrucci, David; Levas, Anthony; Bagchi, Sugato; Gondek, David;
       Mueller, Erik T. (2013-06-01). "Watson: Beyond Jeopardy!".
       Artificial Intelligence. 199: 93–105.
       doi:10.1016/j.artint.2012.06.009.
    3. ^ ^a ^b Hale, Mike (February 8, 2011). "Actors and Their Roles for
       $300, HAL? HAL!". The New York Times. Retrieved February 11, 2011.
    4. ^ "The DeepQA Project". IBM Research. Archived from the original on
       June 29, 2011. Retrieved February 18, 2011.
    5. ^ "Dave Ferrucci at Computer History Museum – How It All Began and
       What's Next". IBM Research. December 1, 2011. Archived from the
       original on March 13, 2012. Retrieved February 11, 2012.
    6. ^ Loftus, Jack (April 26, 2009). "IBM Prepping 'Watson' Computer to
       Compete on Jeopardy!". Gizmodo. Archived from the original on July
       31, 2017. Retrieved September 18, 2017.
    7. ^ "IBM's "Watson" Computing System to Challenge All Time Henry
       Lambert Jeopardy! Champions". Sony Pictures Television. December
       14, 2010. Archived from the original on June 16, 2013.
    8. ^ ^a ^b Upbin, Bruce (February 8, 2013). "IBM's Watson Gets Its
       First Piece Of Business In Healthcare". Forbes. Archived from the
       original on September 18, 2017. Retrieved September 18, 2017.
    9. ^ ^a ^b Ferrucci, D.; et al. (2010). "Building Watson: An Overview
       of the DeepQA Project". AI Magazine. 31 (3): 59.
       doi:10.1609/aimag.v31i3.2303. Archived from the original on
       December 28, 2017. Retrieved February 19, 2011.
   10. ^ "Watson, A System Designed for Answers: The Future of Workload
       Optimized Systems Design". IBM Systems and Technology. February
       2011. p. 3. Archived from the original on March 4, 2016. Retrieved
       September 9, 2015.
   11. ^ ^a ^b Jackson, Joab (February 17, 2011). "IBM Watson Vanquishes
       Human Jeopardy Foes". PC World. IDG News. Archived from the
       original on February 20, 2011. Retrieved February 17, 2011.
   12. ^ Takahashi, Dean (February 17, 2011). "IBM researcher explains
       what Watson gets right and wrong". VentureBeat. Archived from the
       original on February 18, 2011. Retrieved February 18, 2011.
   13. ^ Novell (February 2, 2011). "Watson Supercomputer to Compete on
       'Jeopardy!' – Powered by SUSE Linux Enterprise Server on IBM
       POWER7". The Wall Street Journal. Archived from the original on
       April 21, 2011. Retrieved February 21, 2011.
   14. ^ ^a ^b "Is Watson the smartest machine on earth?". Computer
       Science and Electrical Engineering Department, University of
       Maryland Baltimore County. February 10, 2011. Archived from the
       original on September 27, 2011. Retrieved February 11, 2011.
   15. ^ ^a ^b Rennie, John (February 14, 2011). "How IBM's Watson
       Computer Excels at Jeopardy!". PLoS blogs. Archived from the
       original on February 22, 2011. Retrieved February 19, 2011.
   16. ^ Lucas, Mearian (February 21, 2011). "Can anyone afford an IBM
       Watson supercomputer? (Yes)". Computerworld. Archived from the
       original on December 12, 2013. Retrieved February 21, 2011.
   17. ^ "Top500 List – November 2013". Top500.org. Archived from the
       original on 2013-12-31. Retrieved 2014-01-04.
   18. ^ Ferrucci, David; et al. "The AI Behind Watson – The Technical
       Article". AI Magazine (Fall 2010). Archived from the original on
       November 6, 2020. Retrieved November 11, 2013.
   19. ^ ^a ^b ^c ^d ^e ^f ^g ^h ^i ^j ^k ^l ^m ^n ^o ^p ^q Thompson,
       Clive (June 16, 2010). "Smarter Than You Think: What Is I.B.M.'s
       Watson?". The New York Times Magazine. Archived from the original
       on June 5, 2011. Retrieved February 18, 2011.
   20. ^ "Will Watson Win On Jeopardy!?". Nova ScienceNOW. Public
       Broadcasting Service. January 20, 2011. Archived from the original
       on April 14, 2011. Retrieved January 27, 2011.
   21. ^ ^a ^b ^c ^d Needleman, Rafe (February 18, 2011). "Reporters'
       Roundtable: Debating the robobrains". CNET. Retrieved February 18,
       2011.^[permanent dead link]
   22. ^ Russo-Spena, Tiziana; Mele, Cristina; Marzullo, Marialuisa
       (2018). "Practising Value Innovation through Artificial
       Intelligence: The IBM Watson Case". Journal of Creating Value. 5
       (1): 11–24. doi:10.1177/2394964318805839. ISSN 2394-9643.
       S2CID 56759835.
   23. ^ ^a ^b ^c ^d "Jeopardy! Champ Ken Jennings". The Washington Post.
       February 15, 2011. Archived from the original on February 14, 2011.
       Retrieved February 15, 2011.
   24. ^ ^a ^b Gondek, David (January 10, 2011). "How Watson "sees,"
       "hears," and "speaks" to play Jeopardy!". IBM Research News.
       Retrieved February 21, 2011.
   25. ^ Avery, Lise (February 14, 2011). "Interview with Actor Jeff
       Woodman, Voice of IBM's Watson Computer" (MP3). Anything Goes!!.
       Archived from the original on September 21, 2019. Retrieved
       February 15, 2011.
   26. ^ Kosinski, Robert J. (2008). "A Literature Review on Reaction
       Time". Clemson University. Archived from the original on March 17,
       2016. Retrieved January 10, 2016.
   27. ^ ^a ^b Baker (2011), p. 174.
   28. ^ Baker (2011), p. 178.
   29. ^ ^a ^b ^c Strachan, Alex (February 12, 2011). "For Jennings, it's
       a man vs. man competition". The Vancouver Sun. Archived from the
       original on February 21, 2011. Retrieved February 15, 2011.
   30. ^ Baker (2011), pp. 6–8.
   31. ^ Baker (2011), p. 30.
   32. ^ Radev, Dragomir R.; Prager, John; Samn, Valerie (2000). "Ranking
       potential answers to natural language questions" (PDF). Proceedings
       of the 6th Conference on Applied Natural Language Processing.
       Archived from the original (PDF) on 2011-08-26. Retrieved
       2011-02-23.
   33. ^ Prager, John; Brown, Eric; Coden, Annie; Radev, Dragomir R. (July
       2000). "Question-answering by predictive annotation" (PDF).
       Proceedings, 23rd Annual International ACM SIGIR Conference on
       Research and Development in Information Retrieval. Archived from
       the original (PDF) on 2011-08-23. Retrieved 2011-02-23.
   34. ^ Leopold, George (July 18, 2007). "IBM's Paul Horn retires, Kelly
       named research chief". EE Times. Archived from the original on June
       3, 2020. Retrieved May 27, 2020.
   35. ^ Babcock, Charles (October 14, 2015). "IBM Cognitive Colloquium
       Spotlights Uncovering Dark Data". InformationWeek. Archived from
       the original on June 3, 2020. Retrieved May 27, 2020.
   36. ^ ^a ^b Brodkin, Jon (February 10, 2010). "IBM's Jeopardy-playing
       machine can now beat human contestants". Network World. Archived
       from the original on June 3, 2013. Retrieved February 19, 2011.
   37. ^ Zimmer, Ben (February 17, 2011). "Is It Time to Welcome Our New
       Computer Overlords?". The Atlantic. Archived from the original on
       August 29, 2018. Retrieved February 17, 2011.
   38. ^ Raz, Guy (January 28, 2011). "Can a Computer Become a Jeopardy!
       Champ?". National Public Radio. Archived from the original on
       February 28, 2011. Retrieved February 18, 2011.
   39. ^ "Medical Students Offer Expertise to IBM's Jeopardy!-Winning
       Computer Watson as It Pursues a New Career in Medicine" (PDF).
       InTouch. New York Medical College. 18: 4. June 2012. Archived from
       the original (PDF) on 2012-11-23.
   40. ^ "'Millionaire' quiz whiz Toutant had passion for trivia, Austin's
       arts scene". Archived from the original on 2021-09-23. Retrieved
       2021-09-23.
   41. ^ Stelter, Brian (December 14, 2010). "I.B.M. Supercomputer
       'Watson' to Challenge 'Jeopardy' Stars". The New York Times.
       Retrieved December 14, 2010.
   42. ^ Baker (2011), p. 171.
   43. ^ Flatow, Ira (February 11, 2011). "IBM Computer Faces Off Against
       'Jeopardy' Champs". Talk of the Nation. National Public Radio.
       Archived from the original on February 17, 2011. Retrieved February
       15, 2011.
   44. ^ Sostek, Anya (February 13, 2011). "Human champs of 'Jeopardy!'
       vs. Watson the IBM computer: a close match". Pittsburgh Post
       Gazette. Archived from the original on February 17, 2011. Retrieved
       February 19, 2011.
   45. ^ ^a ^b ^c Jennings, Ken (February 16, 2011). "My Puny Human
       Brain". Slate. Newsweek Interactive Co. LLC. Archived from the
       original on February 18, 2011. Retrieved February 17, 2011.
   46. ^ Baker (2011), p. 117.
   47. ^ Baker (2011), pp. 232–258.
   48. ^ Dignan, Larry (January 13, 2011). "IBM's Watson wins Jeopardy
       practice round: Can humans hang?". ZDnet. Archived from the
       original on January 13, 2011. Retrieved January 13, 2011.
   49. ^ ^a ^b "The IBM Challenge Day 1". Jeopardy. Season 27. Episode 23.
       February 14, 2011.
   50. ^ Lenchner, Jon (February 3, 2011). "Knowing what it knows:
       selected nuances of Watson's strategy". IBM Research News. IBM.
       Archived from the original on February 16, 2011. Retrieved February
       16, 2011.
   51. ^ Johnston, Casey (February 15, 2011). "Jeopardy: IBM's Watson
       almost sneaks wrong answer by Trebek". Ars Technica. Archived from
       the original on February 18, 2011. Retrieved February 15, 2011.
   52. ^ ^a ^b ^c ^d "Computer crushes the competition on 'Jeopardy!'".
       Associated Press. February 15, 2011. Archived from the original on
       February 19, 2011. Retrieved February 19, 2011.
   53. ^ Tesauro, Gerald (February 13, 2011). "Watson's wagering
       strategies". IBM Research News. IBM. Archived from the original on
       February 18, 2011. Retrieved February 18, 2011.
   54. ^ Staff (February 15, 2011). "IBM's computer wins 'Jeopardy!'
       but... Toronto?". CTV News. Archived from the original on November
       27, 2012. Retrieved February 15, 2011.
   55. ^ ^a ^b Robertson, Jordan; Borenstein, Seth (February 16, 2011).
       "For Watson, Jeopardy! victory was elementary". The Globe and Mail.
       The Associated Press. Archived from the original on February 20,
       2011. Retrieved February 17, 2011.
   56. ^ Hamm, Steve (February 15, 2011). "Watson on Jeopardy! Day Two:
       The Confusion over and Airport Clue". A Smart Planet Blog. Archived
       from the original on October 24, 2011. Retrieved February 21, 2011.
   57. ^ Johnston, Casey (February 15, 2011). "Creators: Watson has no
       speed advantage as it crushes humans in Jeopardy". Ars Technica.
       Archived from the original on February 18, 2011. Retrieved February
       21, 2011.
   58. ^ Oberman, Mira (February 17, 2011). "Computer creams human
       Jeopardy! champions". Vancouver Sun. Agence France-Presse. Archived
       from the original on February 20, 2011. Retrieved February 17,
       2011.
   59. ^ Johnston, Casey (February 17, 2011). "Bug lets humans grab Daily
       Double as Watson triumphs on Jeopardy". Ars Technica. Archived from
       the original on February 21, 2011. Retrieved February 21, 2011.
   60. ^ ^a ^b Upbin, Bruce (February 17, 2011). "IBM's Supercomputer
       Watson Wins It All With $367 Bet". Forbes. Archived from the
       original on February 21, 2011. Retrieved February 21, 2011.
   61. ^ Oldenburg, Ann (February 17, 2011). "Ken Jennings: 'My puny
       brain' did just fine on 'Jeopardy!'". USA Today. Archived from the
       original on February 20, 2011. Retrieved February 21, 2011.
   62. ^ "Show 6088 – The IBM Challenge, Day 2". Jeopardy!. February 16,
       2011. Syndicated.
   63. ^ "World Community Grid to benefit from Jeopardy! competition".
       World Community Grid. February 4, 2011. Archived from the original
       on January 14, 2012. Retrieved February 19, 2011.
   64. ^ "Jeopardy! And IBM Announce Charities To Benefit From Watson
       Competition". IBM Corporation. January 13, 2011. Archived from the
       original on November 10, 2021. Retrieved February 19, 2011.
   65. ^ "IBM's Watson supercomputer crowned Jeopardy king". BBC News.
       February 17, 2011. Archived from the original on February 18, 2011.
       Retrieved February 17, 2011.
   66. ^ Markoff, John (February 16, 2011). "Computer Wins on 'Jeopardy!':
       Trivial, It's Not". The New York Times. Yorktown Heights, New York.
       Archived from the original on October 22, 2014. Retrieved February
       17, 2011.
   67. ^ Searle, John (February 23, 2011). "Watson Doesn't Know It Won on
       'Jeopardy!'". The Wall Street Journal. Archived from the original
       on November 10, 2021. Retrieved July 26, 2011.
   68. ^ Lohr, Steve (December 5, 2011). "Creating AI based on the real
       thing". The New York Times. Archived from the original on November
       10, 2021. Retrieved February 26, 2017..
   69. ^ ^a ^b "NJ congressman tops 'Jeopardy' computer Watson".
       Associated Press. March 2, 2011. Archived from the original on
       March 7, 2011. Retrieved March 2, 2011.
   70. ^ Weber, Robert C. (February 14, 2011). "Why 'Watson' matters to
       lawyers". The National Law Journal. Archived from the original on
       November 10, 2021. Retrieved February 18, 2011.
   71. ^ Nay, Chris (September 6, 2011). "Putting Watson to work:
       Interview with GM of Watson Solutions Manoj Saxena". Smarter Planet
       Blog. IBM. Archived from the original on November 12, 2013.
       Retrieved November 12, 2013.
   72. ^ Merritt, Rick (February 14, 2011). "IBM playing Jeopardy with tax
       dollars". EE Times. Archived from the original on February 18,
       2011. Retrieved February 19, 2011.
   73. ^ Dusto, Amy (December 3, 2013). "IBM's Watson computer helps
       shoppers via a new app". Internet Retailer. Archived from the
       original on June 12, 2016. Retrieved January 10, 2016.
   74. ^ Comstock, Jonah (November 15, 2013). "With Watson API launch, IBM
       turns to WellTok for patients, MD Buyline for docs".
       MobiHealthNews. Archived from the original on March 4, 2016.
       Retrieved January 10, 2016.
   75. ^ Upbin, Bruce (November 14, 2013). "IBM Opens Up Its Watson
       Cognitive Computer For Developers Everywhere". Forbes. Archived
       from the original on January 26, 2016. Retrieved January 10, 2016.
   76. ^ "IBM's Watson to Join Research Team at Rensselaer". Rensselaer
       Polytechnic Institute. January 30, 2013. Archived from the original
       on October 4, 2013. Retrieved October 1, 2013.
   77. ^ "The Independent Sector: Cultural, Economic and Social
       Contributions of New York's 100+, Not-for-Profit Colleges and
       Universities" (PDF). Commission on Independent Colleges and
       Universities. Summer 2013. p. 12. Archived (PDF) from the original
       on October 4, 2013. Retrieved October 1, 2013.
   78. ^ Cocks, Tim (February 6, 2014). "IBM starts rolling out Watson
       supercomputer in Africa". Reuters. Archived from the original on
       March 8, 2016. Retrieved January 10, 2016.
   79. ^ Coolidge, Donald (May 29, 2014). "IBM Watson Mobile Developers
       Challenge Finalists: Majestyk". Majestyk Apps. Archived from the
       original on September 19, 2015. Retrieved January 10, 2016.
   80. ^ "Majestyk Apps – An IBM Watson Mobile Developer Challenge
       Winner". Flickr. June 3, 2014. Archived from the original on
       September 27, 2015. Retrieved January 10, 2016.
   81. ^ "Red Ant – An IBM Watson Mobile Developer Challenge Winner".
       Flickr. June 3, 2014. Archived from the original on December 28,
       2015. Retrieved January 10, 2016.
   82. ^ "GenieMD – An IBM Watson Mobile Developer Challenge Winner".
       Flickr. June 3, 2014. Archived from the original on September 22,
       2015. Retrieved January 10, 2016.
   83. ^ "IBM Announces Watson Mobile Developer Challenge Winners". IBM
       News. June 3, 2014. Archived from the original on January 19, 2016.
       Retrieved January 10, 2016.
   84. ^ All, Ann (July 9, 2014). "Genesys to Put IBM's Watson to Work".
       Enterprise Apps Today. Archived from the original on March 7, 2016.
       Retrieved January 10, 2016.
   85. ^ Wilson, Mark (June 30, 2014). "IBM's Watson Is Now A Cooking App
       With Infinite Recipes". fastcodesign.com. Archived from the
       original on February 28, 2016. Retrieved January 10, 2016.
   86. ^ Hardawar, Devindra. "IBM's big bet on Watson is paying off with
       more apps and DNA analysis". Engadget. Archived from the original
       on July 9, 2015. Retrieved July 2, 2015.
   87. ^ Griggs, Mary Beth (May 20, 2016). "IBM Watson can help find water
       wasters in drought-stricken California". Popular Science. Archived
       from the original on June 23, 2016. Retrieved July 4, 2016.
   88. ^ Marty Swant (6 September 2016). "Condé Nast Has Started Using
       IBM's Watson to Find Influencers for Brands". Adweek. Archived from
       the original on 8 September 2016. Retrieved 8 September 2016.
   89. ^ "Rare Carat Releases World's First Artificial Intelligence
       Jeweler Using IBM Watson Technology". PRNewswire. February 28,
       2017. Archived from the original on August 22, 2017. Retrieved
       August 22, 2017.
   90. ^ "Rare Carat's Watson-powered chatbot will help you put a diamond
       ring on it". TechCrunch. February 15, 2017. Archived from the
       original on August 22, 2017. Retrieved August 22, 2017.
   91. ^ "10 ways you may have already used IBM Watson". VentureBeat.
       March 10, 2017. Archived from the original on August 22, 2017.
       Retrieved August 22, 2017.
   92. ^ "IBM Watson to help Pebble Beach create a virtual concierge for
       guests". VentureBeat. 2017-05-09. Archived from the original on
       2017-05-09. Retrieved 2017-05-10.
   93. ^ IBM Watson: artificial intelligence arrives at the Museum of
       Anthropology. Aban Tech. October 31, 2017. Archived from the
       original on 2019-09-21. Retrieved May 11, 2018 – via Youtube.
   94. ^ "IBM Watson is AI for Business".
   95. ^ ^a ^b ^c "Putting Watson to Work: Watson in Healthcare". IBM.
       Archived from the original on November 11, 2013. Retrieved November
       11, 2013.
   96. ^ "IBM Watson Helps Fight Cancer with Evidence-Based Diagnosis and
       Treatment Suggestions" (PDF). IBM. Archived (PDF) from the original
       on April 26, 2013. Retrieved November 12, 2013.
   97. ^ Saxena, Manoj (February 13, 2013). "IBM Watson Progress and 2013
       Roadmap (Slide 7)". IBM. Archived from the original on November 13,
       2013. Retrieved November 12, 2013.
   98. ^ Wakeman, Nick (February 17, 2011). "IBM's Watson heads to medical
       school". Washington Technology. Archived from the original on
       February 21, 2011. Retrieved February 19, 2011.
   99. ^ Mathews, Anna Wilde (September 12, 2011). "Wellpoint's New Hire:
       What is Watson?". The Wall Street Journal. Archived from the
       original on February 22, 2017. Retrieved March 12, 2017.
   100. ^ Miliard, Mike (October 30, 2012). "Watson Heads to Medical
       School: Cleveland Clinic, IBM Send Supercomputer to College".
       Healthcare IT News. Archived from the original on November 11,
       2013. Retrieved November 11, 2013.
   101. ^ "MD Anderson Taps IBM Watson to Power "Moon Shots" Mission Aimed
       at Ending Cancer, Starting with Leukemia" (Press release). IBM.
       Archived from the original on 2017-02-21. Retrieved 2017-02-20.
   102. ^ "IBM's Watson Now Tackles Clinical Trials At MD Anderson Cancer
       Center". Forbes. Archived from the original on 2017-09-20.
       Retrieved 2017-09-18.
   103. ^ "MD Anderson Benches IBM Watson In Setback For Artificial
       Intelligence In Medicine". Forbes. Archived from the original on
       2017-10-02. Retrieved 2017-09-18.
   104. ^ Leske, Nikola (February 9, 2013). "Doctors Seek Help on Cancer
       Treatment from IBM Supercomputer". Reuters. Archived from the
       original on February 16, 2016. Retrieved November 11, 2013.
   105. ^ ^a ^b "Manipal Hospitals | Watson for Oncology | Cancer
       Treatment". watsononcology.manipalhospitals.com. Archived from the
       original on 2017-01-18. Retrieved 2017-01-17.
   106. ^ "MANIPAL HOSPITALS ANNOUNCES NATIONAL LAUNCH OF IBM WATSON FOR
       ONCOLOGY". www-03.ibm.com. 2016-07-29. Archived from the original
       on 2017-01-18. Retrieved 2017-01-17.
   107. ^ "Manipal Hospitals is first adopter of IBM Watson in India".
       www-03.ibm.com. 2015-12-02. Archived from the original on
       2017-01-18. Retrieved 2017-01-17.
   108. ^ ANI (2016-10-28). "Manipal Hospitals to adopt IBM's 'Watson for
       Oncology' supercomputer for cancer treatment". Business Standard
       India. Archived from the original on 2017-01-18. Retrieved
       2017-01-17.
   109. ^ "Hospitals in Asia use Watson supercomputer for cancer
       treatment". STAT. 2016-08-19. Archived from the original on
       2017-01-18. Retrieved 2017-01-17.
   110. ^ McCurry, Justin (2017-01-05). "Japanese company replaces office
       workers with artificial intelligence". The Guardian.
       ISSN 0261-3077. Archived from the original on 2019-05-03. Retrieved
       2017-01-29.
   111. ^ Garbuio, Massimo; Lin, Nidthida (2019). "Artificial Intelligence
       as a Growth Engine for Health Care Startups: Emerging Business
       Models". California Management Review. 61 (2): 59–83.
       doi:10.1177/0008125618811931. S2CID 158219917.
   112. ^ Cooper, Laura (2021-02-18). "IBM Explores Sales of IBM Watson
       Health".
   113. ^ Meil, Doug (2021-04-21). "What Happened To Watson Health?".
       Association for Computing Machinery:BLOG@CACM. Retrieved
       2021-04-21.
   114. ^ Cooper, Laura (2021-02-18). "IBM explores sale of IBM Watson
       Health".
   115. ^ "IBM is selling off its Watson Health assets". CNN. 2022.
       Retrieved 6 February 2022.
   116. ^ ^a ^b ^c "IBM Watson Group Unveils Cloud-Delivered Watson
       Services to Transform Industrial R&D, Visualize Big Data Insights
       and Fuel Analytics Exploration" (Press release). IBM. January 9,
       2014. Archived from the original on October 12, 2020. Retrieved
       February 14, 2020.
   117. ^ Ante, Spencer E. (January 9, 2014). "IBM Set to Expand Watson's
       Reach". The Wall Street Journal. Archived from the original on May
       9, 2015. Retrieved January 9, 2014.
   118. ^ "Inside the Lab". September 2017. Archived from the original on
       October 23, 2020. Retrieved October 6, 2020.
   119. ^ "IBM CEO Rometty Proposes 'Watson's Law': AI In Everything"
       Archived 2021-04-16 at the Wayback Machine, Adrian Bridgewater,
       Forbes, March 20, 2018
   120. ^ Pelley, Lauren (2 February 2015). "Poutine inventions offer new
       wheys to eat your curds". Toronto Star. Toronto, Ontario. pp. E1,
       E8. ProQuest 1650084755.
   121. ^ Kleeman, Alexandra. "Cooking with Chef Watson, I.B.M.'s
       Artificial-Intelligence App". The New Yorker. Archived from the
       original on 25 January 2021. Retrieved 18 April 2021.
   122. ^ Takahashi, Dean (23 February 2015). "Elemental's smart connected
       toy CogniToys taps IBM's Watson supercomputer for its brains".
       Venture Beat. Archived from the original on 20 May 2015. Retrieved
       15 May 2015.
   123. ^ "About Us GoFetchCode". GoFetchCode. 2015-10-21. Archived from
       the original on 2017-07-10. Retrieved 2017-07-04.
   124. ^ Leopold, Todd. "A professor built an AI teaching assistant for
       his courses — and it could shape the future of education". Business
       Insider. Business Insider. Archived from the original on 26
       September 2017. Retrieved 26 September 2017.
   125. ^ Straumsheim, Carl. "'Augmented Intelligence' for Higher Ed".
       Inside Higher Ed. Inside Higher Ed. Archived from the original on
       27 September 2017. Retrieved 26 September 2017.
   126. ^ Plenty, Rebecca (October 25, 2016). "Pearson Taps IBM's Watson
       as a Virtual Tutor for College Students". No. October 25, 2016.
       Bloomberg. Bloomberg. Archived from the original on September 27,
       2017. Retrieved 26 September 2017.
   127. ^ Maderer, Jason. "Artificial Intelligence Course Creates AI
       Teaching Assistant". Georgia Tech News. Georgia Tech News. Archived
       from the original on 29 July 2018. Retrieved 26 September 2017.
   128. ^ McFarlane, Matt (13 May 2016). "Professor reveals to students
       that his assistant was an AI all along". Sydney Morning Herald.
       Archived from the original on May 16, 2016. Retrieved May 14, 2016.
   129. ^ Memeti, Suejb; Pllana, Sabri (January 2018). "PAPA: A parallel
       programming assistant powered by IBM Watson cognitive computing
       technology". Journal of Computational Science. 26: 275–284.
       doi:10.1016/j.jocs.2018.01.001.
   130. ^ Jancer, Matt (26 August 2016). "IBM's Watson Takes On Yet
       Another Job, as a Weather Forecaster". Smithsonian. Archived from
       the original on 1 September 2016. Retrieved 29 August 2016.
   131. ^ Booton, Jennifer (15 June 2016). "IBM finally reveals why it
       bought The Weather Company". Market Watch. Archived from the
       original on 22 August 2016. Retrieved 29 August 2016.
   132. ^ "Cognitive dress by Marchesa and IBM lights up the night at the
       Met Gala". Business Operations. 2016-10-27. Archived from the
       original on 2020-06-13. Retrieved 2020-07-06.
   133. ^ Moscaritolo, Angela (2 February 2017). "H&R Block Enlists IBM
       Watson to Find Tax Deductions". PC Magazine. Archived from the
       original on 15 February 2017. Retrieved 14 February 2017.
   134. ^ "Artificial Intelligence Marketing Company". Cognitiv. Retrieved
       2022-08-23.
   135. ^ Swant, Marty (September 24, 2017). "As IBM Ramps Up Its
       AI-Powered Advertising, Can Watson Crack the Code of Digital
       Marketing?". www.adweek.com. Archived from the original on
       2019-04-29. Retrieved 2019-03-18.
   136. ^ "AI is A Rocket About to Launch - Here's How to Get On Board".
       www.ibm.com. Archived from the original on 2019-10-29. Retrieved
       2019-03-18.

Bibliography[edit]

     *

   Baker, Stephen (2011). Final Jeopardy: Man vs. Machine and the Quest to
   Know Everything. Boston, New York: Houghton Mifflin Harcourt.
   ISBN 978-0-547-48316-0.

Further reading[edit]

     * Baker, Stephen (2012) Final Jeopardy: The Story of Watson, the
       Computer That Will Transform Our World, Mariner Books.
     * Jackson, Joab (2014). IBM bets big on Watson-branded cognitive
       computing PCWorld: Jan 9, 2014 2:30 PM
     * Greenemeier, Larry. (2013). Will IBM's Watson Usher in a New Era of
       Cognitive Computing? Scientific American. Nov 13, 2013 |* Lazarus,
       R. S. (1982).
     * Kelly, J.E. and Hamm, S. ( 2013). Smart Machines: IBM's Watson and
       the Era of Cognitive Computing. Columbia Business School Publishing

External links[edit]

   This article's use of external links may not follow Wikipedia's
   policies or guidelines. Please improve this article by removing
   excessive or inappropriate external links, and converting useful links
   where appropriate into footnote references. (May 2022) (Learn how and
   when to remove this template message)
   Wikimedia Commons has media related to IBM Watson.
     * Watson homepage
     * DeepQA homepage
     * About Watson on Jeopardy.com
     * Smartest Machine on Earth (PBS NOVA documentary about the making of
       Watson)
     * Power Systems
     * The Watson Trivia Challenge. The New York Times. June 16, 2010.
     * This is Watson – IBM Journal of Research and Development (published
       by the IEEE)

J! Archive[edit]

     * Jeopardy! Show #6086 – Game 1, Part 1
     * Jeopardy! Show #6087 – Game 1, Part 2
     * Jeopardy! Show #6088 – Game 2

Videos[edit]

     * PBS NOVA documentary on the making of Watson
     * Building Watson – A Brief Overview of the DeepQA Project on YouTube
       (21:42), IBMLabs
     * How Watson Answers a Question on YouTube
     * David Ferrucci, Dan Cerutti and Ken Jennings on IBM's Watson at
       Singularity Summit 2011 on YouTube
     * A Computer Called Watson on YouTube – November 15, 2011, David
       Ferrucci at Computer History Museum, alternate
     * IBM Watson and the Future of Healthcare on YouTube – 2012
     * IBM Watson-Introduction and Future Applications on YouTube – IBM at
       EDGE 2012
     * IBM Watson for Healthcare on YouTube – Martin Kohn, 2013
     * IBM Watson playlist, IBMLabs Watson playlist

     * v
     * t
     * e

   IBM

   History

     * History
     * Mergers and acquisitions
          + PC business acquisition by Lenovo

   Products

   Hardware
   Current
     * Mainframe
          + IBM Z
     * Power microprocessors
     * Power Systems
     * Storage
          + FlashSystem
          + DS8000
          + Q System One

   Former
     * Blue Gene
     * Cell microprocessors
     * PowerPC
     * Midrange computer
     * Personal Computer
     * Selectric
     * ThinkPad

   Other
     * alphaWorks
     * Carbon Design System
     * Cloud
          + Cloudant
     * Cognos Analytics
     * Connections
     * Criminal Reduction Utilising Statistical History
     * Fortran
     * ILOG
     * Information Management Software
     * Lotus Software
     * Mainframe operating systems
     * Mashup Center
     * Planning Analytics
     * PureQuery
     * Quantum Experience
     * Rational Software
     * SPSS
     * Tivoli Software
          + Service Automation Manager
     * Watson
     * WebSphere

   Business
   entities

   Current
     * Center for The Business of Government
     * Consulting
     * Red Hat
     * Kenexa
     * International subsidiaries
     * Research
     * The Weather Company (Weather Underground)

   Former
     * AdStar
     * AIM alliance
     * Ambra Computer
     * EduQuest
     * Kaleida Labs
     * Kyndryl
     * Merative
     * Microelectronics
     * Product Center
     * Service Bureau
     * Taligent

   Facilities

     * Towers
          + 1250 René-Lévesque, Montreal, QC
          + One Atlantic Center, Atlanta, GA
     * Software Labs
          + Rome Software Lab
          + Toronto Software Lab
     * IBM Buildings
          + 330 North Wabash, Chicago, IL
          + Honolulu
          + Seattle
     * Facilities
          + Thomas J. Watson Research Center
          + Hakozaki Facility
          + Yamato Facility
     * Cambridge Scientific Center
     * IBM Hursley
     * Canada Head Office Building
     * IBM Rochester

   Initiatives

     * Academy of Technology
     * Deep Thunder
     * Developer
          + Develothon
     * Fellow
     * The Great Mind Challenge
     * Linux Technology Center
     * Smarter Planet
     * Virtual Universe Community
     * World Community Grid

   Inventions

     * Automated teller machine
     * Cynefin framework
     * DRAM
     * Electronic keypunch
     * Floppy disk
     * Hard disk drive
     * Magnetic stripe card
     * Relational model
     * Sabre airline reservation system
     * Scanning tunneling microscope
     * Financial swaps
     * Universal Product Code

   Terminology

     * Big Blue
     * Commercial Processing Workload
     * Customer engineer
     * Globally integrated enterprise
     * e-business
     * Think slogan

   CEOs

     * Thomas J. Watson (1914–1956)
     * Thomas Watson Jr. (1956–1971)
     * T. Vincent Learson (1971–1973)
     * Frank T. Cary (1973–1981)
     * John R. Opel (1981–1985)
     * John Fellows Akers (1985–1993)
     * Louis V. Gerstner Jr. (1993–2002)
     * Samuel J. Palmisano (2002–2011)
     * Ginni Rometty (2012–2020)
     * Arvind Krishna (since 2020)

   Board of
   directors

     * Thomas Buberl
     * Michael L. Eskew
     * David Farr
     * Alex Gorsky
     * Michelle J. Howard
     * Arvind Krishna
     * Andrew N. Liveris
     * Martha E. Pollack
     * Virginia M. Rometty
     * Joseph R. Swedish
     * Sidney Taurel
     * Peter R. Voser

   Other

     * A Boy and His Atom
     * Big Blue sports teams
          + American football
          + Rugby union
     * Common Public License/IBM Public License
     * Deep Blue
     * Deep Thought
     * Dynamic infrastructure
     * GlobalFoundries
     * GUIDE International
     * IBM and the Holocaust
     * International chess tournament
     * Lucifer cipher
     * Mathematica
     * IBM Plex
     * SHARE computing
     * ScicomP
     * Worker organization

     * Category
     * Commons
     * Navigational boxes
          + FOSS
          + Midrange computers
          + Operating systems
          + Personal computers
          + System/360
          + System/370
          + Typewriters
          + Vacuum tube computers

     * v
     * t
     * e

   Jeopardy!

   Hosts

     * Art Fleming
     * Alex Trebek
     * Mike Richards
     * Mayim Bialik
     * Ken Jennings

   Concepts

     * Audition process
     * Broadcast information
     * Strategies and skills of Jeopardy! champions

   Tournaments

   Current
   Syndicated
     * Tournament of Champions
     * Second Chance Tournament
     * Professors Tournament

   Primetime
     * National College Championship
     * Celebrity Jeopardy!
     * Jeopardy! Masters

   Past
   Syndicated
     * International Tournament of Champions (1996–2001)
     * College Championship (1989–2020)
     * Teen Tournament (1987–2019)
     * Kids Week (1999–2014)
     * Reunion tournaments
     * Celebrity (1992–2015)
     * Power Players Week (1997–2016)

   Special
     * Tenth Anniversary Tournament (1993)
     * Million Dollar Masters (2002)
     * Ultimate Tournament of Champions (2005)
     * Million Dollar Celebrity Invitational (2009–10)
     * IBM Challenge (2011)
     * Battle of the Decades (2014)
     * All-Star Games (2019)

   Primetime
     * Super Jeopardy! (1990)
     * The Greatest of All Time (2020)

   Notable
   contestants

     * Matt Amodio
     * Colby Burnett
     * Arthur Chu
     * Buzzy Cohen
     * Julia Collins
     * Richard Cordray
     * Karl Coryat
     * Roger Craig
     * Chuck Forrest
     * Jackie Fuchs
     * Red Gibson
     * Bob Harris
     * James Holzhauer
     * Matt Jackson
     * Alex Jacob
     * Ken Jennings
     * Larissa Kelly
     * Mark Lowenthal
     * David Madden
     * Arthur Phillips
     * Mattea Roach
     * Brad Rutter
     * Amy Schneider
     * Frank Spangenberg
     * Eddie Timanus
     * Jerome Vered
     * Watson
     * Jay Wolpert

   Franchise

     * Jep!
     * Rock & Roll Jeopardy!
     * Sports Jeopardy!
     * British version

   In popular
   culture

     * Saturday Night Live sketch series
     * "I Lost on Jeopardy"
     * Ellen's Energy Adventure
     * Cliff Clavin
     * Velma Dinkley
     * Fran Fine
     * Thelma Harper
     * Shaggy Rogers
     * Marge Simpson
     * Adam West
     * Dorothy Zbornak

     * Category

     * v
     * t
     * e

   Virtual assistants

   Active

     * AliGenie
     * Alexa
     * Alice
     * Bixby
          + Viv
     * Braina
     * Celia
     * Clova
     * Cortana
     * Google Assistant
     * Maluuba
     * Mycroft
     * Siri
     * Voice Mate
     * Watson
     * WolframAlpha
     * Xiaoice

   Discontinued

     * BlackBerry Assistant
     * Google Now
     * M
     * Microsoft Agent
     * Microsoft Bob
     * Microsoft Voice Command
     * Ms. Dewey
     * Mya
     * Office Assistant (Clippy)
     * S Voice
     * Speaktoit Assistant
     * Tafiti
     * Vlingo

     * v
     * t
     * e

   Differentiable computing

   General

     * Differentiable programming
     * Information geometry
     * Statistical manifold

     * Automatic differentiation
     * Neuromorphic engineering
     * Pattern recognition
     * Tensor calculus
     * Computational learning theory
     * Inductive bias

   Concepts

     * Gradient descent
          + SGD
     * Clustering
     * Regression
          + Overfitting
     * Hallucination
     * Adversary
     * Attention
     * Convolution
     * Loss functions
     * Backpropagation
     * Normalization (Batchnorm)
     * Activation
          + Softmax
          + Sigmoid
          + Rectifier
     * Regularization
     * Datasets
          + Augmentation
     * Diffusion
     * Autoregression

   Applications

     * Machine learning
          + In-context learning
     * Artificial neural network
          + Deep learning
     * Scientific computing
     * Artificial Intelligence
     * Language model
          + Large language model

   Hardware

     * IPU
     * TPU
     * VPU
     * Memristor
     * SpiNNaker

   Software libraries

     * TensorFlow
     * PyTorch
     * Keras
     * Theano
     * JAX
     * LangChain

   Implementations

   Audio–visual
     * AlexNet
     * WaveNet
     * Human image synthesis
     * HWR
     * OCR
     * Speech synthesis
     * Speech recognition
     * Facial recognition
     * AlphaFold
     * DALL-E
     * Midjourney
     * Stable Diffusion

   Verbal
     * Word2vec
     * Seq2seq
     * BERT
     * LaMDA
          + Bard
     * NMT
     * Project Debater
     * IBM Watson
     * GPT-2
     * GPT-3
     * ChatGPT
     * GPT-4
     * GPT-J
     * Chinchilla AI
     * PaLM
     * BLOOM
     * LLaMA

   Decisional
     * AlphaGo
     * AlphaZero
     * Q-learning
     * SARSA
     * OpenAI Five
     * Self-driving car
     * MuZero
     * Action selection
          + Auto-GPT
     * Robot control

   People

     * Yoshua Bengio
     * Alex Graves
     * Ian Goodfellow
     * Stephen Grossberg
     * Demis Hassabis
     * Geoffrey Hinton
     * Yann LeCun
     * Fei-Fei Li
     * Andrew Ng
     * Jürgen Schmidhuber
     * David Silver

   Organizations

     * Anthropic
     * EleutherAI
     * Google DeepMind
     * Hugging Face
     * OpenAI
     * Meta AI
     * Mila
     * MIT CSAIL

   Architectures

     * Neural Turing machine
     * Differentiable neural computer
     * Transformer
     * Recurrent neural network (RNN)
     * Long short-term memory (LSTM)
     * Gated recurrent unit (GRU)
     * Echo state network
     * Multilayer perceptron (MLP)
     * Convolutional neural network
     * Residual network
     * Autoencoder
     * Variational autoencoder (VAE)
     * Generative adversarial network (GAN)
     * Graph neural network

     * Portals
          + Computer programming
          + Technology
     * Categories
          + Artificial neural networks
          + Machine learning

   Retrieved from
   "https://en.wikipedia.org/w/index.php?title=IBM_Watson&oldid=1162661401
   "

   Categories:
     * Computer-related introductions in 2006
     * IBM cloud services
     * IBM computers
     * Jeopardy! contestants
     * Natural language processing software
     * One-of-a-kind computers
     * Virtual assistants

   Hidden categories:
     * All articles with dead external links
     * Articles with dead external links from January 2023
     * Articles with permanently dead external links
     * Webarchive template wayback links
     * Articles with short description
     * Short description matches Wikidata
     * All articles with unsourced statements
     * Articles with unsourced statements from April 2022
     * Articles with a promotional tone from April 2019
     * All articles with a promotional tone
     * Wikipedia external links cleanup from May 2022
     * Wikipedia spam cleanup from May 2022
     * Commons category link is on Wikidata

     * This page was last edited on 30 June 2023, at 12:47 (UTC).
     * Text is available under the Creative Commons Attribution-ShareAlike
       License 4.0; additional terms may apply. By using this site, you
       agree to the Terms of Use and Privacy Policy. Wikipedia® is a
       registered trademark of the Wikimedia Foundation, Inc., a
       non-profit organization.

     * Privacy policy
     * About Wikipedia
     * Disclaimers
     * Contact Wikipedia
     * Code of Conduct
     * Mobile view
     * Developers
     * Statistics
     * Cookie statement

     * Wikimedia Foundation
     * Powered by MediaWiki

   (BUTTON) Toggle limited content width
